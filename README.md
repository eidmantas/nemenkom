# Waste Schedule System

A system for scraping, storing, and displaying waste pickup schedules from `nemenkom.lt` (NemenÄinÄ— municipality, Lithuania).

## Features

- **XLSX Scraper**: Downloads and parses waste schedule spreadsheets
- **Hybrid Parser**: Traditional regex parser + AI (Groq) for complex location patterns
- **SQLite Database**: Efficient storage with hash-based schedule grouping
- **REST API**: Flask-based API for data access
- **Web Interface**: User-friendly interface for viewing schedules
- **Google Calendar Integration**: (Planned) Generate calendar events

## Quick Start

### Docker/Podman (Recommended)

**Microservice Architecture:**
- `web` service: Flask API and web interface (port 3333)
- `scraper` service: Scheduled scraper (runs at 11:00 and 18:00 daily)

**Using Makefile (recommended):**
```bash
make up      # Start all services
make down    # Stop services
make restart # Restart services
make build   # Build images
make clean   # Stop and remove everything
make test    # Run tests locally
```

**Or directly with podman-compose:**
```bash
# Start all services
podman-compose up -d

# View logs (all services)
podman-compose logs -f

# View logs (specific service)
podman-compose logs -f scraper
podman-compose logs -f web

# Stop all services
podman-compose down
```

Web server: **http://localhost:3333**

The database is stored in `./database/` and persists between restarts. The scraper automatically updates it twice daily.

### Option 2: Manual Setup

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Initialize database
python database/init.py
```

### Run Scraper

```bash
# Simple subset only (traditional parser, no AI needed)
python scraper/main.py --simple-subset

# Full parsing (when AI parser is implemented)
python scraper/main.py
```

### Run Web Server

```bash
python api/app.py
```

Server runs on **http://localhost:3333**

## API Testing

### List All Locations
```bash
curl http://localhost:3333/api/v1/locations
```

### Search Locations
```bash
curl "http://localhost:3333/api/v1/locations?q=Aleksandravas"
```

### Get Schedule for Location
```bash
curl "http://localhost:3333/api/v1/schedule?location_id=1"
```

### Get Schedule Group Info
```bash
curl "http://localhost:3333/api/v1/schedule-group/sg_f5f4eff319af?waste_type=bendros"
```

## Website Design

### Current Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BuitiniÅ³ atliekÅ³ surinkimo grafikas    â”‚
â”‚  [Search: IeÅ¡koti gatvÄ—s ar kaimo...]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GatvÄ—s ir kaimai â”‚   Kalendorius        â”‚
â”‚  (Left Panel)     â”‚   (Right Panel)      â”‚
â”‚                   â”‚                      â”‚
â”‚  â€¢ Aleksandravas  â”‚   [Selected Location]â”‚
â”‚  â€¢ AukÅ¡tadvaris   â”‚   [Calendar View]    â”‚
â”‚  â€¢ ...            â”‚   [Pickup Dates]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Features
- **Search**: Filter locations by village/street name
- **Location List**: Click to select and view schedule
- **Calendar View**: Displays pickup dates for selected location
- **Responsive**: Basic responsive design (mobile improvements planned)

### Future Enhancements
- **Cascading Selection UI**: Village â†’ Street â†’ House Number (when house numbers are in DB)
  - Auto-populating dropdowns: Select village â†’ streets filter â†’ house numbers filter
  - "Visiems" (All) option when no specific house numbers exist
- Google Calendar export button
- Multi-waste-type selection (general, plastic, glass)
- Address input with autocomplete
- Improved mobile UI

## Project Structure

```
nemenkom/
â”œâ”€â”€ scraper/          # Data scraping and parsing
â”‚   â”œâ”€â”€ fetcher.py    # Download XLSX from URL
â”‚   â”œâ”€â”€ parser.py     # Parse XLSX (traditional + AI router)
â”‚   â”œâ”€â”€ parser_router.py  # Decide traditional vs AI parsing
â”‚   â”œâ”€â”€ validator.py  # Validate data structure
â”‚   â”œâ”€â”€ db_writer.py  # Write to SQLite
â”‚   â””â”€â”€ main.py       # CLI entry point
â”œâ”€â”€ api/              # Flask REST API
â”‚   â”œâ”€â”€ app.py        # Flask application
â”‚   â””â”€â”€ db.py         # Database queries
â”œâ”€â”€ web/              # Web interface
â”‚   â”œâ”€â”€ templates/    # HTML templates
â”‚   â””â”€â”€ static/       # CSS, JS
â”œâ”€â”€ database/         # Database schema and init
â”‚   â”œâ”€â”€ schema.sql    # SQLite schema
â”‚   â””â”€â”€ init.py       # Database initialization
â””â”€â”€ documentation/    # Project documentation
```

## Database Schema

### Key Tables

- **`schedule_groups`**: Hash-based groups with JSON dates and kaimai_hashes
- **`locations`**: Village/street combinations with kaimai_hash
- **`data_fetches`**: Track scraping runs

See `database/schema.sql` for full schema.

## Next Steps

### 1. Implement AI Parser (Groq)
- Create `scraper/ai_parser.py`
- Integrate Groq API for complex "Kaimai" patterns
- Add rate limiting (30 RPM, 14,400 RPD free tier)
- Update `parser.py` to use AI parser via router

### 2. Add Scraper Service to Docker Compose âœ…
- âœ… Separate service with dedicated Dockerfile.scraper
- âœ… Runs at 11:00 and 18:00 daily via cron
- âœ… Microservice architecture: web + scraper services

### 3. Google Calendar Integration
- Refactor `google_calendar.py`
- Add API endpoint: `POST /api/v1/generate-calendars`
- Generate events per schedule group

### 4. Multi-Waste-Type Support
- Handle separate XLSX files for plastic, glass waste
- Update parser to accept `waste_type` parameter
- Update API to filter by waste type

### 5. Enhanced Web Interface (House Numbers Support)
- **Cascading Selection**: Village â†’ Street â†’ House Number
  - Step 1: Select City/Village (dropdown)
  - Step 2: Select Street (filtered by selected village, auto-populated)
  - Step 3: Select House Number (filtered by selected street, auto-populated)
  - Show "Visiems" (All) option when no specific house numbers exist
- Update API to support filtering by house numbers
- Update database queries to handle house number filtering

### 6. Testing & Deployment
- Test AI parser with full dataset
- Verify date accuracy across all locations
- Production deployment

## Documentation

- **`documentation/ARCHITECTURE.md`** - System architecture and design
- **`documentation/HYBRID_PARSER.md`** - Parser implementation details
- **`documentation/AI_COST_ANALYSIS.md`** - AI options cost analysis
- **`documentation/DECISION_SCHEDULE_GROUPS.md`** - Database schema decisions
- **`documentation/AI-AGENT.md`** - Full context for AI agents

## Development Notes

### Current Status
- âœ… Database schema implemented (hash-based IDs, JSON dates)
- âœ… Traditional parser working (simple patterns)
- âœ… Parser router implemented
- âœ… API and web interface functional
- ğŸš§ AI parser (next step)
- ğŸš§ Google Calendar integration
- ğŸš§ Multi-waste-type support

### Testing
- Use `--simple-subset` flag to test traditional parser only
- Database currently has 900 locations, 10 schedule groups (simple subset)
- Verify dates match XLSX source data

